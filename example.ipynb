{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reference_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['Imagenet large scale visual recognition challenge 2013 (ilsvrc2013'],\n",
       " 'url': ['http://www.imagenet.org/challenges/LSVRC/2013/.'],\n",
       " 'type': None,\n",
       " 'cite_id': '1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_path = \"papers/1403.6382v3.pdf\"\n",
    "\n",
    "# make a temporary directory for processing\n",
    "working_dir = make_working_dir()\n",
    "\n",
    "# get raw text from pdf files\n",
    "text = parse_pdf2text(paper_path)\n",
    "\n",
    "# extract raw reference for raw text\n",
    "reference_list = extract_references_and_citations(text)\n",
    "\n",
    "# parse raw reference to structure json data\n",
    "parsed_refs = parse_references(reference_list, working_dir)\n",
    "\n",
    "parsed_refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cite_id</th>\n",
       "      <th>tf-idf_score</th>\n",
       "      <th>ref_title</th>\n",
       "      <th>res_title</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>Imagenet large scale visual recognition challe...</td>\n",
       "      <td>ImageNet Large Scale Visual Recognition Challenge</td>\n",
       "      <td>http://dx.doi.org/10.1007/s11263-015-0816-y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Efficient object detection and segmentation fo...</td>\n",
       "      <td>Efficient Object Detection and Segmentation fo...</td>\n",
       "      <td>http://dx.doi.org/10.1109/cvpr.2013.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>All about VLAD</td>\n",
       "      <td>All About VLAD</td>\n",
       "      <td>http://dx.doi.org/10.1109/cvpr.2013.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.711559</td>\n",
       "      <td>Poof: Part-based one-vs.-onefeatures for fine-...</td>\n",
       "      <td>POOF: Part-Based One-vs.-One Features for Fine...</td>\n",
       "      <td>http://dx.doi.org/10.1109/cvpr.2013.128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cite_id  tf-idf_score                                          ref_title  \\\n",
       "0       1      0.776515  Imagenet large scale visual recognition challe...   \n",
       "1       2      1.000000  Efficient object detection and segmentation fo...   \n",
       "3       4      1.000000                                     All about VLAD   \n",
       "4       5      0.711559  Poof: Part-based one-vs.-onefeatures for fine-...   \n",
       "\n",
       "                                           res_title  \\\n",
       "0  ImageNet Large Scale Visual Recognition Challenge   \n",
       "1  Efficient Object Detection and Segmentation fo...   \n",
       "3                                     All About VLAD   \n",
       "4  POOF: Part-Based One-vs.-One Features for Fine...   \n",
       "\n",
       "                                           URL  \n",
       "0  http://dx.doi.org/10.1007/s11263-015-0816-y  \n",
       "1      http://dx.doi.org/10.1109/cvpr.2013.110  \n",
       "3      http://dx.doi.org/10.1109/cvpr.2013.207  \n",
       "4      http://dx.doi.org/10.1109/cvpr.2013.128  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search reference title by crossref\n",
    "retrieve_df = retrieve_from_crossref(parsed_refs[:5])\n",
    "# filter those titles that not similar to the query titles\n",
    "retrieve_df = retrieve_df[retrieve_df[\"tf-idf_score\"] > 0.7]\n",
    "\n",
    "retrieve_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'LTTextBoxHorizontal',\n",
       " 'page': 1,\n",
       " 'bbox': [0.5046764705882353,\n",
       "  0.1502690383838385,\n",
       "  0.8907109418300657,\n",
       "  0.42149706868686887],\n",
       " 'text': 'quickly ﬁnd out the answer”. But when the convolutional\\nneural network OverFeat [38] was recently made pub-\\nlicly available1 it allowed for some experimentation.\\nIn\\nparticular we wondered now, not whether one could train\\na deep network speciﬁcally for a given task, but if the fea-\\ntures extracted by a deep network - one carefully trained\\non the diverse ImageNet database to perform the speciﬁc\\ntask of image classiﬁcation - could be exploited for a wide\\nvariety of vision tasks. We now relate our discussions and\\ngeneral ﬁndings because as a computer vision researcher\\nyou’ve probably had the same questions:\\nProf: First off has anybody else investigated this issue?\\nStudent: Well it turns out Donahue et al. [10], Zeiler\\nand Fergus [48] and Oquab et al. [29] have suggested that\\ngeneric features can be extracted from large CNNs and pro-\\nvided some initial evidence to support this claim. But they\\nhave only considered a small number of visual recognition\\ntasks. It would be fun to more thoroughly investigate how',\n",
       " 'citations': ['38', '10', '48', '29'],\n",
       " 'citation_sentences': [' But when the convolutional\\nneural network OverFeat [38] was recently made pub-\\nlicly available1 it allowed for some experimentation',\n",
       "  ' [10], Zeiler\\nand Fergus [48] and Oquab et al',\n",
       "  ' [10], Zeiler\\nand Fergus [48] and Oquab et al',\n",
       "  ' [29] have suggested that\\ngeneric features can be extracted from large CNNs and pro-\\nvided some initial evidence to support this claim']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the text boxes that have citations in the main text\n",
    "ciatation_boxes = extract_element_boxes(paper_path, get_citation=True)\n",
    "ciatation_boxes[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
